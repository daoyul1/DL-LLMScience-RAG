{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":54662,"databundleVersionId":6169864,"sourceType":"competition"},{"sourceId":5632975,"sourceType":"datasetVersion","datasetId":3238926},{"sourceId":6146260,"sourceType":"datasetVersion","datasetId":3521629},{"sourceId":6359012,"sourceType":"datasetVersion","datasetId":3662908},{"sourceId":6476221,"sourceType":"datasetVersion","datasetId":3741139},{"sourceId":6536614,"sourceType":"datasetVersion","datasetId":3778974},{"sourceId":6602268,"sourceType":"datasetVersion","datasetId":3809212},{"sourceId":6641274,"sourceType":"datasetVersion","datasetId":3833789},{"sourceId":6641614,"sourceType":"datasetVersion","datasetId":3834047},{"sourceId":6641618,"sourceType":"datasetVersion","datasetId":3834048},{"sourceId":10223364,"sourceType":"datasetVersion","datasetId":6320129},{"sourceId":10224607,"sourceType":"datasetVersion","datasetId":6321045},{"sourceId":10230718,"sourceType":"datasetVersion","datasetId":6325590},{"sourceId":6537899,"sourceType":"datasetVersion","datasetId":3779689}],"dockerImageVersionId":30805,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install dependencies from Kaggle datasets\n!pip install -U --no-deps /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!pip install -U --no-deps /kaggle/input/datasets-214/datasets-2.14.5-py3-none-any.whl\n!pip install -U --no-deps /kaggle/input/optimum-113/optimum-1.13.2-py3-none-any.whl\n!pip install --no-deps /kaggle/input/tokenizers-0-13-3/tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!pip install -U --no-deps /kaggle/input/transformers-432/transformers-4.32.1-py3-none-any.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:04:55.222167Z","iopub.execute_input":"2024-12-18T22:04:55.222813Z","iopub.status.idle":"2024-12-18T22:08:02.368247Z","shell.execute_reply.started":"2024-12-18T22:04:55.222774Z","shell.execute_reply":"2024-12-18T22:08:02.367059Z"}},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\nProcessing /kaggle/input/datasets-214/datasets-2.14.5-py3-none-any.whl\nInstalling collected packages: datasets\n  Attempting uninstall: datasets\n    Found existing installation: datasets 3.1.0\n    Uninstalling datasets-3.1.0:\n      Successfully uninstalled datasets-3.1.0\nSuccessfully installed datasets-2.14.5\nProcessing /kaggle/input/optimum-113/optimum-1.13.2-py3-none-any.whl\nInstalling collected packages: optimum\nSuccessfully installed optimum-1.13.2\nProcessing /kaggle/input/tokenizers-0-13-3/tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nInstalling collected packages: tokenizers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.20.3\n    Uninstalling tokenizers-0.20.3:\n      Successfully uninstalled tokenizers-0.20.3\nSuccessfully installed tokenizers-0.13.3\nProcessing /kaggle/input/transformers-432/transformers-4.32.1-py3-none-any.whl\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.46.3\n    Uninstalling transformers-4.46.3:\n      Successfully uninstalled transformers-4.46.3\nSuccessfully installed transformers-4.32.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Loading Dependencies","metadata":{}},{"cell_type":"code","source":"import gc\nimport logging\nimport ctypes\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nfrom pathlib import Path\nfrom time import time\nfrom functools import partial\nfrom concurrent.futures import ThreadPoolExecutor\nfrom threading import Condition\nfrom tqdm.auto import tqdm\n\nimport faiss\nfrom torch.utils.data import DataLoader\nfrom datasets import load_from_disk, Dataset\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, AutoModel\nfrom accelerate import init_empty_weights\nfrom accelerate.utils.modeling import set_module_tensor_to_device\nfrom safetensors.torch import load_file\nfrom optimum.bettertransformer import BetterTransformer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:08:02.370490Z","iopub.execute_input":"2024-12-18T22:08:02.370790Z","iopub.status.idle":"2024-12-18T22:08:28.692836Z","shell.execute_reply.started":"2024-12-18T22:08:02.370760Z","shell.execute_reply":"2024-12-18T22:08:28.691554Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  torch.utils._pytree._register_pytree_node(\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Set up hyperparameters & helper function","metadata":{}},{"cell_type":"code","source":"NUM_TITLES = 5\nMAX_SEQ_LEN = 512\n# Using bge-faiss-index\n# EMBED_MODEL_DIR = \"/kaggle/input/bge-faiss-index-llmscience/\"\nEMBED_MODEL_DIR = \"/kaggle/input/bge-small-faiss/\"\n\n# Using all-mpnet-base-faiss index\n# EMBED_MODEL_DIR = \"/kaggle/input/all-mpnet-base-v2-faiss-llmscience/\"\nMAX_TOKENS = 4096\nMAX_CONTEXT_TOKENS = 1200\nN_BATCHES = 5\n\ndef clear_memory():\n    \"\"\"Frees up unused memory on both CPU and GPU.\"\"\"\n    gc.collect()\n    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n    torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:08:28.694341Z","iopub.execute_input":"2024-12-18T22:08:28.695240Z","iopub.status.idle":"2024-12-18T22:08:28.701318Z","shell.execute_reply.started":"2024-12-18T22:08:28.695164Z","shell.execute_reply":"2024-12-18T22:08:28.700373Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# LLM and RAG pipeline","metadata":{}},{"cell_type":"code","source":"class SentenceEmbedder:\n    \"\"\"\n    Produces embeddings for given sentences using a transformer-based model.\n    This is functionally similar to the original SentenceTransformer logic.\n    We need to implement it since the LLMScience is Internet off, so we cannot download the SentenceTransformer.\n    \"\"\"\n    def __init__(self, model_dir, device=\"cuda:0\"):\n        self.device = device\n        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n        self.model = AutoModel.from_pretrained(model_dir).half().to(device)\n\n    def _preprocess(self, text_batch):\n        return self.tokenizer(\n            text_batch[\"text\"], \n            truncation=True, \n            padding=True, \n            return_tensors=\"pt\", \n            max_length=MAX_SEQ_LEN\n        ).to(self.device)\n\n    def _dataloader(self, sentences, batch_size=32):\n        decorated = [\"Represent this sentence for searching relevant passages: \" + s for s in sentences]\n        dataset = Dataset.from_dict({\"text\": decorated})\n        dataset.set_transform(self._preprocess)\n        return DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n    def encode(self, texts, show_progress=False, batch_size=32):\n        loader = self._dataloader(texts, batch_size)\n        iterator = tqdm(loader) if show_progress else loader\n        all_embeddings = []\n        with torch.no_grad():\n            for batch in iterator:\n                embeddings = self.model(**batch).pooler_output\n                normalized = F.normalize(embeddings, p=2, dim=1).cpu().numpy()\n                all_embeddings.append(normalized)\n        return np.concatenate(all_embeddings, axis=0)\n\n\nclass WeightsSynchronizer:\n    \"\"\"\n    Manages synchronous loading of model layers (in safetensors format)\n    across multiple devices to ensure all devices load the same layer simultaneously.\n    \"\"\"\n    def __init__(self, base_path, device_list):\n        self.checkpoint_path = Path(base_path)\n        self.state_lock = Condition()\n        self.states = {d: None for d in device_list}\n        self.cached_state_dict = None\n\n    def request_weights(self, layer_name, device):\n        with self.state_lock:\n            self.states[device] = layer_name\n            if all(self.states.values()):\n                # Confirm all devices are requesting the same layer\n                assert len(set(self.states.values())) == 1, \"Mismatch in requested layer names\"\n                layer_file = self.checkpoint_path / (layer_name + \".safetensors\")\n                self.cached_state_dict = load_file(layer_file, device=\"cpu\")\n                for dvc in self.states:\n                    self.states[dvc] = None\n                self.state_lock.notify_all()\n\n    def retrieve_state_dict(self, device):\n        with self.state_lock:\n            while self.states[device] is not None:\n                self.state_lock.wait()\n            result = self.cached_state_dict\n            self.states[device] = None\n            if not any(self.states.values()):\n                self.state_lock.notify_all()\n            return result\n\n\nclass LayeredLlama:\n    \"\"\"\n    Loads and runs a large model in a sharded manner. \n    Layers are loaded on-demand to minimize memory usage.\n    \"\"\"\n    def __init__(self, model_path, weight_sync, device=\"cuda:0\", dtype=torch.float16):\n        self.ckpt_path = Path(model_path)\n        self.weight_sync = weight_sync\n        self.device = device\n        self.dtype = dtype\n\n        # Load configuration and tokenizer\n        self.config = AutoConfig.from_pretrained(self.ckpt_path)\n        self.tokenizer = AutoTokenizer.from_pretrained(self.ckpt_path)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.tokenizer.padding_side = \"right\"\n\n        # Build a model skeleton without weights\n        self._create_empty_model()\n        # Store layer names for sequential loading\n        self.layer_sequence = [\"model.embed_tokens\"] + \\\n                              [f\"model.layers.{i}\" for i in range(len(self.model.model.layers))] + \\\n                              [\"model.norm\", \"value_head\"]\n\n    def _create_empty_model(self):\n        with init_empty_weights():\n            self.model = AutoModelForCausalLM.from_config(self.config)\n            self.model.lm_head = torch.nn.Linear(8192, 8, bias=False)  # Adjusted head dimension\n            self.model.eval()\n            self.model = BetterTransformer.transform(self.model)\n            self.model.tie_weights()\n\n        # Move buffers to device\n        for name, buffer in self.model.named_buffers():\n            set_module_tensor_to_device(self.model, name, self.device, value=buffer, dtype=self.dtype)\n\n        self.layers = ([self.model.model.embed_tokens] + list(self.model.model.layers) + \n                       [self.model.model.norm, self.model.lm_head])\n\n    def _load_and_place(self, layer_name):\n        self.weight_sync.request_weights(layer_name, self.device)\n        weights = self.weight_sync.retrieve_state_dict(self.device)\n        # Handle final layer rename if necessary\n        if \"value_head.weight\" in weights:\n            weights = {\"lm_head.weight\": weights[\"value_head.weight\"]}\n\n        for param_name, param in weights.items():\n            set_module_tensor_to_device(self.model, param_name, self.device, value=param, dtype=self.dtype)\n\n    def __call__(self, batched_data):\n        # Reset model and free memory before processing\n        del self.model\n        clear_memory()\n        self._create_empty_model()\n\n        # Prepare data\n        batches = [(pfx.to(self.device), sfx.to(self.device)) for pfx, sfx in batched_data]\n        suffix_count = len(batches[0][1])\n        suffix_ends = [(sfx != self.tokenizer.pad_token_id).sum(1) - 1 for _, sfx in batches]\n\n        # Generate large attention mask and position IDs\n        attn_mask = (torch.ones(MAX_TOKENS, MAX_TOKENS).triu(diagonal=1)[None, None, ...] == 0).to(self.device)\n        pos_ids = torch.arange(MAX_TOKENS, dtype=torch.long, device=self.device)[None, :]\n\n        # Execute layers in sequence with threading for async loading\n        with ThreadPoolExecutor() as executor, torch.inference_mode():\n            pending = executor.submit(self._load_and_place, \"model.embed_tokens\")\n\n            for idx, (lname, layer) in tqdm(enumerate(zip(self.layer_sequence, self.layers)), desc=self.device, total=len(self.layers)):\n                pending.result()  # Ensure weights are loaded\n                if (idx + 1) < len(self.layer_sequence):\n                    pending = executor.submit(self._load_and_place, self.layer_sequence[idx + 1])\n\n                # Forward pass through the current layer\n                for i, (prefix, suffix) in enumerate(batches):\n                    if lname == \"model.embed_tokens\":\n                        # Initial embedding\n                        batches[i] = (layer(prefix), layer(suffix))\n                    elif lname == \"model.norm\":\n                        # Take the final token from suffix\n                        final_tokens = suffix[torch.arange(suffix_count), suffix_ends[i]][:, None]\n                        batches[i] = (None, layer(final_tokens))\n                    elif lname == \"value_head\":\n                        # Compute mean of the output across suffix dimension\n                        batches[i] = layer(suffix)[:, 0].mean(1).cpu().numpy()\n                    else:\n                        plen, slen = prefix.shape[1], suffix.shape[1]\n\n                        # Process prefix with kv-cache\n                        pre_out, (k_cache, v_cache) = layer(prefix, use_cache=True, \n                                                             attention_mask=attn_mask[:, :, -plen:, -plen:])\n                        # Process suffix using expanded cache\n                        pos = pos_ids[:, plen:plen + slen].expand(suffix_count, -1)\n                        att = attn_mask[:, :, -slen:, -plen - slen:].expand(suffix_count, -1, -1, -1)\n                        kv_cache = (k_cache.expand(suffix_count, -1, -1, -1), v_cache.expand(suffix_count, -1, -1, -1))\n                        sfx_out = layer(suffix, past_key_value=kv_cache, position_ids=pos, attention_mask=att)[0]\n\n                        batches[i] = (pre_out, sfx_out)\n\n                # Move layer back to meta to save memory\n                layer.to(\"meta\")\n                clear_memory()\n\n        return batches\n\n\ndef load_and_prepare_data():\n    # Load test data from CSV\n    df = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\", index_col=\"id\")\n    is_test = (len(df) != 200)\n    return df, is_test\n\n\ndef embed_prompts(df):\n    start_time = time()\n    print(f\"Initializing embeddings at t={time() - start_time:.1f}s\")\n\n    embedding_model = SentenceEmbedder(EMBED_MODEL_DIR, device=\"cuda:0\")\n\n    def combine_text(row):\n        return \" \".join([row[\"prompt\"], row[\"A\"], row[\"B\"], row[\"C\"], row[\"D\"], row[\"E\"]])\n    combined_inputs = df.apply(combine_text, axis=1).values\n\n    prompt_emb = embedding_model.encode(combined_inputs, show_progress=False)\n    return prompt_emb, start_time\n\n\n# Using FAISS index\ndef retrieve_context(df, prompt_emb, start_t):\n    print(f\"Loading FAISS index at t={time() - start_t:.1f}s\")\n    faiss_idx = faiss.read_index(EMBED_MODEL_DIR + 'faiss.index')\n    # faiss_idx = faiss.read_index(EMBED_MODEL_DIR + '/bge-small-faiss.index')\n    # faiss_idx = faiss.read_index(EMBED_MODEL_DIR + '/all-mpnet-base-v2-faiss.index')\n    # faiss_idx = faiss.read_index('/kaggle/input/all-mp-net-base-v2-embedings/wikipedia_embs_768_all-mp-net-base-v2_faiss.index')\n\n    print(f\"Performing vector search at t={time() - start_t:.1f}s\")\n    results = faiss_idx.search(np.float32(prompt_emb), NUM_TITLES)[1]\n\n    print(f\"Loading dataset for retrieval at t={time() - start_t:.1f}s\")\n    ds = load_from_disk(\"/kaggle/input/all-paraphs-parsed-expanded\")\n    for i in range(len(df)):\n        df.loc[i, \"context\"] = \"-\" + \"\\n-\".join([ds[int(idx)][\"text\"] for idx in results[i]])\n\n    faiss_idx.reset()\n    del faiss_idx, prompt_emb, ds\n    clear_memory()\n    print(f\"Context retrieval complete at t={time() - start_t:.1f}s\")\n\n# NOT Using FAISS index\n# def retrieve_context(df, prompt_emb, start_t):\n#     print(f\"Loading dataset for retrieval at t={time() - start_t:.1f}s\")\n#     ds = load_from_disk(\"/kaggle/input/all-paraphs-parsed-expanded\")\n    \n#     # Instead of using FAISS results, we can simply pick no context or a dummy context.\n#     # For a fair comparison, I justjust leave these as empty lines.\n#     for i in range(len(df)):\n#         df.loc[i, \"context\"] = \"\"\n\n#     del prompt_emb, ds\n#     clear_memory()\n#     print(f\"Context retrieval complete at t={time() - start_t:.1f}s\")\n\n\ndef generate_symlinks():\n    # Recreate model directory structure\n    base_path = Path(\"/root/.cache/\")\n    base_path.mkdir(exist_ok=True, parents=True)\n\n    for part_num in [1, 2, 3]:\n        src_dir = Path(f\"/kaggle/input/platypus2-chuhac2-part{part_num}\")\n        for item in src_dir.glob(\"*\"):\n            link_path = base_path / item.name\n            if not link_path.exists():\n                link_path.symlink_to(item)\n    return base_path\n\n\ndef prepare_inputs(row, tok):\n    sys_prompt = (\n        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n        \"Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instr}\\n\\n### Input:\\nContext:\\n{ctx}\"\n    )\n    instruction = (\"Your task is to analyze the question and answer below. If the answer is correct, respond yes, \"\n                   \"if it is not correct respond no. As a potential aid to your answer, background context from \"\n                   \"Wikipedia articles is at your disposal, even if they might not always be relevant.\")\n\n    # Suffix prompts for each answer option\n    suffix_opts = [f\"{row[ch]}\\n\\n### Response:\\n\" for ch in \"ABCDE\"]\n    suffix_ids = tok(suffix_opts, return_tensors=\"pt\", truncation=True, max_length=MAX_TOKENS, padding=True)[\"input_ids\"][:, 1:]\n\n    question_str = f\"\\nQuestion: {row['prompt']}\\nProposed answer: \"\n    question_ids = tok(question_str, return_tensors=\"pt\", truncation=True, max_length=max(0, MAX_TOKENS - suffix_ids.shape[1]))[\"input_ids\"][:, 1:]\n\n    # Context tokens\n    context_str = sys_prompt.format(instr=instruction, ctx=row[\"context\"])\n    ctx_len = min(MAX_CONTEXT_TOKENS, max(0, MAX_TOKENS - question_ids.shape[1] - suffix_ids.shape[1]))\n    context_ids = tok(context_str, return_tensors=\"pt\", truncation=True, max_length=ctx_len)[\"input_ids\"]\n\n    prefix_tokens = torch.cat([context_ids, question_ids], dim=1)\n    return prefix_tokens, suffix_ids\n\n\ndef run_inference(df, model_path):\n    # Determine devices and initialize weights synchronization\n    gpu_list = [f\"cuda:{i}\" for i in range(torch.cuda.device_count())]\n    weights_sync = WeightsSynchronizer(model_path, gpu_list)\n\n    # Split DataFrame and run model in parallel\n    def process_partition(device, sub_df):\n        model = LayeredLlama(model_path, weights_sync, device=device)\n        tokenize_fn = partial(prepare_inputs, tok=model.tokenizer)\n        data_inputs = sub_df.apply(tokenize_fn, axis=1).values\n        subsets = np.array_split(data_inputs, N_BATCHES)\n        results = []\n        for chunk in subsets:\n            results += model(chunk)\n        return results\n\n    with ThreadPoolExecutor() as pool:\n        parts = np.array_split(df, 2)\n        results = list(pool.map(process_partition, gpu_list, parts))\n        final_out = sum(results, [])\n\n    # Get prediction order (A,B,C,D,E) from the returned scores\n    for i, scores in enumerate(final_out):\n        ordering = np.argsort(scores)[::-1]\n        df.loc[i, \"prediction\"] = \" \".join([\"ABCDE\"[j] for j in ordering])\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:08:28.703155Z","iopub.execute_input":"2024-12-18T22:08:28.703848Z","iopub.status.idle":"2024-12-18T22:08:28.814702Z","shell.execute_reply.started":"2024-12-18T22:08:28.703803Z","shell.execute_reply":"2024-12-18T22:08:28.813999Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Running","metadata":{}},{"cell_type":"code","source":"def main():\n    df, IS_TEST = load_and_prepare_data()\n    embeddings, start = embed_prompts(df)\n    retrieve_context(df, embeddings, start)\n    model_dir = generate_symlinks()\n\n    if IS_TEST:\n        run_inference(df, model_dir)\n    else:\n        # If not the test set scenario, produce a placeholder submission\n        df[\"prediction\"] = \"A B C\"\n\n    df[[\"prediction\"]].to_csv(\"submission.csv\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T22:08:28.816674Z","iopub.execute_input":"2024-12-18T22:08:28.817424Z","iopub.status.idle":"2024-12-18T22:09:29.422933Z","shell.execute_reply.started":"2024-12-18T22:08:28.817382Z","shell.execute_reply":"2024-12-18T22:09:29.422059Z"}},"outputs":[{"name":"stdout","text":"Initializing embeddings at t=0.0s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  torch.utils._pytree._register_pytree_node(\n/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:479: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(checkpoint_file, map_location=map_location)\nParameter 'transform'=<bound method SentenceEmbedder._preprocess of <__main__.SentenceEmbedder object at 0x7c56b8d23820>> of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n","output_type":"stream"},{"name":"stdout","text":"Loading FAISS index at t=4.3s\nPerforming vector search at t=31.2s\nLoading dataset for retrieval at t=33.7s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n  table = cls._concat_blocks(blocks, axis=0)\n","output_type":"stream"},{"name":"stdout","text":"Context retrieval complete at t=60.5s\n","output_type":"stream"}],"execution_count":5}]}